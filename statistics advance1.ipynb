{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPS06ERNB7y8ITkp19MtSGx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["The F-distribution is a continuous probability distribution that arises primarily in the context of statistical hypothesis testing, particularly in analysis of variance (ANOVA), regression analysis, and comparing variances. The properties of the F-distribution are outlined below:\n","\n","1. Definition:\n","The F-distribution is the ratio of two independent chi-squared distributed random variables, each divided by their respective degrees of freedom. Specifically, if\n","ð‘‹\n","1\n","âˆ¼\n","ðœ’\n","ð‘‘\n","1\n","2\n","X\n","1\n","â€‹\n"," âˆ¼Ï‡\n","d\n","1\n","â€‹\n","\n","2\n","â€‹\n","  and\n","ð‘‹\n","2\n","âˆ¼\n","ðœ’\n","ð‘‘\n","2\n","2\n","X\n","2\n","â€‹\n"," âˆ¼Ï‡\n","d\n","2\n","â€‹\n","\n","2\n","â€‹\n","  (where\n","ðœ’\n","2\n","Ï‡\n","2\n","  represents the chi-squared distribution and\n","ð‘‘\n","1\n","d\n","1\n","â€‹\n","  and\n","ð‘‘\n","2\n","d\n","2\n","â€‹\n","  are their respective degrees of freedom), the random variable:\n","ð¹\n","=\n","ð‘‹\n","1\n","ð‘‘\n","1\n","ð‘‹\n","2\n","ð‘‘\n","2\n","F=\n","d\n","2\n","â€‹\n","\n","X\n","2\n","â€‹\n","\n","â€‹\n","\n","d\n","1\n","â€‹\n","\n","X\n","1\n","â€‹\n","\n","â€‹\n","\n","â€‹\n","\n","follows an F-distribution with\n","ð‘‘\n","1\n","d\n","1\n","â€‹\n","  and\n","ð‘‘\n","2\n","d\n","2\n","â€‹\n","  degrees of freedom (for the numerator and denominator, respectively). These degrees of freedom determine the shape of the F-distribution.\n","\n","2. Shape:\n","The shape of the F-distribution depends on the degrees of freedom of the numerator\n","ð‘‘\n","1\n","d\n","1\n","â€‹\n","  and the denominator\n","ð‘‘\n","2\n","d\n","2\n","â€‹\n"," . Generally:\n","The distribution is skewed right (positively skewed), meaning it has a long tail to the right, particularly for small values of\n","ð‘‘\n","1\n","d\n","1\n","â€‹\n","  and\n","ð‘‘\n","2\n","d\n","2\n","â€‹\n"," .\n","As\n","ð‘‘\n","1\n","d\n","1\n","â€‹\n","  and\n","ð‘‘\n","2\n","d\n","2\n","â€‹\n","  increase, the distribution becomes more symmetric and approaches a normal distribution.\n","The F-distribution is non-negative, as it represents a ratio of squared quantities (chi-squared variables), so it cannot take negative values.\n","3. Mean and Variance:\n","The mean of the F-distribution is:\n","ðœ‡\n","ð¹\n","=\n","ð‘‘\n","2\n","ð‘‘\n","2\n","âˆ’\n","2\n",",\n","for\n","ð‘‘\n","2\n",">\n","2\n","Î¼\n","F\n","â€‹\n"," =\n","d\n","2\n","â€‹\n"," âˆ’2\n","d\n","2\n","â€‹\n","\n","â€‹\n"," ,ford\n","2\n","â€‹\n"," >2\n","The variance of the F-distribution is:\n","ðœŽ\n","ð¹\n","2\n","=\n","2\n","ð‘‘\n","2\n","2\n","(\n","ð‘‘\n","1\n","+\n","ð‘‘\n","1\n","âˆ’\n","2\n",")\n","ð‘‘\n","1\n","(\n","ð‘‘\n","2\n","âˆ’\n","2\n",")\n","2\n","(\n","ð‘‘\n","2\n","âˆ’\n","4\n",")\n",",\n","for\n","ð‘‘\n","2\n",">\n","4\n","Ïƒ\n","F\n","2\n","â€‹\n"," =\n","d\n","1\n","â€‹\n"," (d\n","2\n","â€‹\n"," âˆ’2)\n","2\n"," (d\n","2\n","â€‹\n"," âˆ’4)\n","2d\n","2\n","2\n","â€‹\n"," (d\n","1\n","â€‹\n"," +d\n","1\n","â€‹\n"," âˆ’2)\n","â€‹\n"," ,ford\n","2\n","â€‹\n"," >4\n","Note: If the degrees of freedom for the denominator are too small (e.g.,\n","ð‘‘\n","2\n","â‰¤\n","2\n","d\n","2\n","â€‹\n"," â‰¤2), the mean and variance are not defined.\n","4. Probability Density Function (PDF):\n","The PDF of the F-distribution is given by:\n","\n","ð‘“\n","(\n","ð‘¥\n",";\n","ð‘‘\n","1\n",",\n","ð‘‘\n","2\n",")\n","=\n","(\n","ð‘‘\n","1\n","ð‘¥\n",")\n","ð‘‘\n","1\n","(\n","ð‘‘\n","2\n",")\n","ð‘‘\n","2\n","ð‘¥\n","â‹…\n","ðµ\n","(\n","ð‘‘\n","1\n","2\n",",\n","ð‘‘\n","2\n","2\n",")\n","for\n","ð‘¥\n",">\n","0\n","f(x;d\n","1\n","â€‹\n"," ,d\n","2\n","â€‹\n"," )=\n","xâ‹…B(\n","2\n","d\n","1\n","â€‹\n","\n","â€‹\n"," ,\n","2\n","d\n","2\n","â€‹\n","\n","â€‹\n"," )\n","(d\n","2\n","â€‹\n"," )\n","d\n","2\n","â€‹\n","\n","\n","(d\n","1\n","â€‹\n"," x)\n","d\n","1\n","â€‹\n","\n","\n","â€‹\n","\n","â€‹\n","\n","â€‹\n"," forx>0\n","where\n","ðµ\n","(\n","â‹…\n",",\n","â‹…\n",")\n","B(â‹…,â‹…) is the Beta function, and\n","ð‘‘\n","1\n","d\n","1\n","â€‹\n","  and\n","ð‘‘\n","2\n","d\n","2\n","â€‹\n","  are the degrees of freedom for the numerator and denominator, respectively.\n","\n","5. Cumulative Distribution Function (CDF):\n","The CDF gives the probability that an F-distributed random variable is less than or equal to a particular value\n","ð‘¥\n","x, denoted\n","ð‘ƒ\n","(\n","ð¹\n","â‰¤\n","ð‘¥\n",")\n","P(Fâ‰¤x). The CDF for the F-distribution is typically computed numerically due to its complexity.\n","\n","6. Applications:\n","Hypothesis Testing: The F-distribution is widely used in testing the equality of variances (e.g., the F-test for comparing two population variances) and in ANOVA (to test the equality of means across multiple groups).\n","\n","ANOVA: In ANOVA, the F-statistic is the ratio of the variance between groups to the variance within groups. The value of this statistic follows an F-distribution under the null hypothesis of equal means.\n","\n","Regression: In multiple regression analysis, the F-test is used to test whether at least one of the predictors is significantly related to the dependent variable.\n","\n","7. Degrees of Freedom:\n","The F-distribution has two parameters:\n","Numerator degrees of freedom (df1 or\n","ð‘‘\n","1\n","d\n","1\n","â€‹\n"," ): Corresponds to the degrees of freedom of the chi-squared variable in the numerator.\n","Denominator degrees of freedom (df2 or\n","ð‘‘\n","2\n","d\n","2\n","â€‹\n"," ): Corresponds to the degrees of freedom of the chi-squared variable in the denominator.\n","The shape of the distribution is sensitive to these degrees of freedom. For example, for small degrees of freedom, the distribution is more skewed, and as the degrees of freedom increase, the distribution approaches a normal distribution.\n","\n","8. Asymptotic Properties:\n","As the degrees of freedom increase, the F-distribution becomes more symmetric and resembles a normal distribution. In fact, for large\n","ð‘‘\n","1\n","d\n","1\n","â€‹\n","  and\n","ð‘‘\n","2\n","d\n","2\n","â€‹\n"," , the F-distribution approximates a normal distribution.\n","\n","9. Relation to Other Distributions:\n","Chi-squared distribution: Since the F-distribution is the ratio of two independent chi-squared variables (each divided by its degrees of freedom), it can be viewed as a transformation of the chi-squared distribution.\n","T-distribution: The F-distribution is related to the square of the t-distribution. Specifically, if\n","ð‘¡\n","âˆ¼\n","ð‘¡\n","ð‘‘\n","2\n","tâˆ¼t\n","d\n","2\n","â€‹\n","\n","â€‹\n","  (a t-distribution with\n","ð‘‘\n","2\n","d\n","2\n","â€‹\n","  degrees of freedom), then\n","ð¹\n","=\n","ð‘¡\n","2\n","F=t\n","2\n","  follows an F-distribution with\n","ð‘‘\n","1\n","=\n","1\n","d\n","1\n","â€‹\n"," =1 and\n","ð‘‘\n","2\n","d\n","2\n","â€‹\n","  degrees of freedom.\n","\n","10. Critical Values:\n","Critical values from the F-distribution are used to decide whether to reject the null hypothesis in hypothesis testing. For a given significance level (e.g.,\n","ð›¼\n","0.05\n","Î±=0.05) and degrees of freedom\n","ð‘‘\n","1\n","d\n","1\n","â€‹\n","  and\n","ð‘‘\n","2\n","d\n","2\n","â€‹\n"," , the critical value is the value of\n","ð¹\n","F such that the probability of observing an F-statistic larger than that value is less than\n","ð›¼\n","Î±."],"metadata":{"id":"_bGVYNDyXj3Z"}},{"cell_type":"markdown","source":["The F-distribution is used primarily in statistical tests that involve comparing variances or testing hypotheses related to the variability of data across different groups. The F-distribution is appropriate for these tests because it describes the ratio of two scaled chi-squared variables, which are directly related to variance and variability. Below are the key statistical tests that use the F-distribution and an explanation of why the F-distribution is suitable for these tests.\n","\n","1. Analysis of Variance (ANOVA)\n","\n","Purpose: ANOVA is used to compare the means of two or more groups to determine if there are any statistically significant differences among them.\n","\n","Why F-distribution is used: ANOVA tests the null hypothesis that the group means are equal by comparing the variance between groups to the variance within groups. The F-statistic is the ratio of these two variances:\n","\n","ð¹\n","=\n","VarianceÂ betweenÂ groups\n","VarianceÂ withinÂ groups\n","F=\n","VarianceÂ withinÂ groups\n","VarianceÂ betweenÂ groups\n","â€‹\n","\n","If the null hypothesis is true (i.e., the means are equal), the variance between groups should be similar to the variance within groups, and thus the F-statistic should be close to 1. If the F-statistic is significantly larger than 1, it suggests that the between-group variance is greater than the within-group variance, leading to the rejection of the null hypothesis.\n","\n","Since the ratio of variances follows an F-distribution under the null hypothesis, the F-distribution is the appropriate distribution for calculating the p-value and testing the hypothesis in ANOVA.\n","\n","Types of ANOVA:\n","\n","One-Way ANOVA: Compares the means of more than two groups based on one factor.\n","Two-Way ANOVA: Involves two independent factors and assesses their individual and interactive effects on the dependent variable.\n","Repeated Measures ANOVA: Used when the same subjects are used for multiple treatments or time points.\n","2. F-test for Comparing Two Variances\n","\n","Purpose: The F-test is used to compare the variances of two populations to determine if they are equal.\n","\n","Why F-distribution is used: The F-test involves the ratio of two independent sample variances:\n","ð¹\n","=\n","ð‘ \n","1\n","2\n","ð‘ \n","2\n","2\n","F=\n","s\n","2\n","2\n","â€‹\n","\n","s\n","1\n","2\n","â€‹\n","\n","â€‹\n","\n","where\n","ð‘ \n","1\n","2\n","s\n","1\n","2\n","â€‹\n","  and\n","ð‘ \n","2\n","2\n","s\n","2\n","2\n","â€‹\n","  are the sample variances from two independent groups. Under the null hypothesis that the population variances are equal, this ratio follows an F-distribution with degrees of freedom\n","ð‘‘\n","1\n","d\n","1\n","â€‹\n","  and\n","ð‘‘\n","2\n","d\n","2\n","â€‹\n","  corresponding to the sample sizes of the two groups. If the calculated F-statistic is significantly large, it suggests that the variances are not equal, leading to the rejection of the null hypothesis.\n","The F-distribution is appropriate here because variances are estimated from chi-squared distributions, and the ratio of two independent chi-squared variables (each divided by its respective degrees of freedom) follows an F-distribution.\n","\n","3. Regression Analysis (F-test for Overall Significance)\n","\n","Purpose: In the context of multiple regression, the F-test is used to test the overall significance of the regression modelâ€”whether at least one of the predictors has a statistically significant relationship with the dependent variable.\n","\n","Why F-distribution is used: The F-statistic in regression compares the model sum of squares (variation explained by the regression model) to the residual sum of squares (unexplained variation, or error). The F-statistic is given by:\n","ð¹\n","=\n","ModelÂ MeanÂ Square\n","ErrorÂ MeanÂ Square\n","F=\n","ErrorÂ MeanÂ Square\n","ModelÂ MeanÂ Square\n","â€‹\n","\n","Under the null hypothesis (that all regression coefficients are zero, meaning the predictors have no effect), the F-statistic follows an F-distribution. A large F-statistic indicates that the model explains a significant amount of variance in the dependent variable, suggesting that at least one of the predictors is significant.\n","4. Testing for Homogeneity of Variances (Levene's Test and Bartlett's Test)\n","Purpose: These tests check whether the variances across several groups are equal (i.e., homogeneity of variances), which is a key assumption in many statistical tests like ANOVA and t-tests.\n","\n","Why F-distribution is used: Levene's test and Bartlett's test both involve comparing variances, and the test statistic can follow an F-distribution under the null hypothesis of equal variances across the groups. In these tests, the variance of the data within each group is compared to the overall variance to assess whether there is significant heterogeneity in variances across groups. The F-distribution is appropriate because it is the distribution of the ratio of two variances, which is exactly the type of comparison these tests are making.\n","\n","5. Nested Models and Likelihood Ratio Tests (in Complex Models)\n","Purpose: The F-test can be used in likelihood ratio tests to compare nested models in situations where one model is a special case of another (e.g., a model with fewer predictors is nested within a model with more predictors).\n","\n","Why F-distribution is used: The F-statistic in this case tests whether the reduction in residual sum of squares (or the increase in explained variance) is statistically significant when adding more predictors to the model. The test is based on comparing the goodness-of-fit of two models, and the distribution of the test statistic follows an F-distribution under the null hypothesis that the simpler model is adequate.\n","\n","6. Multivariate Analysis of Variance (MANOVA)\n","Purpose: MANOVA is an extension of ANOVA used when there are multiple dependent variables. It tests whether the means of the dependent variables differ across groups.\n","\n","Why F-distribution is used: In MANOVA, the F-statistic is used to test whether the variance-covariance matrices of the groups are significantly different. Like ANOVA, the F-distribution is used to compare the between-group variance (considering multiple dependent variables) to the within-group variance, allowing for the assessment of group differences in multiple dimensions.\n","Why the F-distribution is Appropriate for These Tests:\n","The F-distribution is appropriate for these tests because:\n","\n","Ratio of Variances: In many of these tests (ANOVA, F-tests for variances, regression analysis), the test statistic is a ratio of two sample variances or sums of squares. Since variances are estimates derived from chi-squared distributions, their ratio follows an F-distribution under the null hypothesis.\n","Non-Negative Values: Variance is always non-negative, so the F-distribution, which is also non-negative, is a natural choice for testing hypotheses related to variance.\n","Skewed Distribution: The F-distribution is positively skewed, which reflects the fact that variances (and thus their ratios) tend to be more variable when they are small, especially when the sample sizes are small or the null hypothesis is true."],"metadata":{"id":"EIXu4fRVYV57"}},{"cell_type":"markdown","source":["To conduct an F-test to compare the variances of two populations, certain assumptions must be met for the test to be valid and the results to be interpretable. The key assumptions are as follows:\n","\n","1. Independence of Samples\n","The two samples being compared must be independent of each other. This means that the observations in one sample should have no influence on the observations in the other sample.\n","For example, if you are comparing the variances of two different groups (e.g., males and females), the observations from the male group should not be related to those from the female group.\n","2. Normality of the Populations\n","Both populations from which the samples are drawn should be normally distributed (or at least approximately normal).\n","The F-test assumes that the data from each group come from a normal distribution because the test is based on the ratio of two sample variances, and sample variances are based on sums of squared deviations from the mean. When the data are normally distributed, the sampling distributions of the variances will follow a chi-squared distribution, which underlies the F-distribution.\n","If the data are not normally distributed, the F-test may not perform well, especially with small sample sizes. For large samples, however, the Central Limit Theorem can sometimes mitigate non-normality, but normality is still an important assumption for small sample sizes.\n","3. Independent Sampling from Normal Distributions (No Outliers)\n","In addition to normality, there should be no extreme outliers in the data. Outliers can disproportionately affect the variance, which can distort the test results. Outliers can lead to an inflated or deflated value of the sample variance, thereby impacting the F-statistic and making the results unreliable.\n","4. Ratio of Variances\n","The F-test compares the ratio of the two sample variances:\n","ð¹\n","=\n","ð‘ \n","1\n","2\n","ð‘ \n","2\n","2\n","F=\n","s\n","2\n","2\n","â€‹\n","\n","s\n","1\n","2\n","â€‹\n","\n","â€‹\n","\n","where\n","ð‘ \n","1\n","2\n","s\n","1\n","2\n","â€‹\n","  and\n","ð‘ \n","2\n","2\n","s\n","2\n","2\n","â€‹\n","  are the sample variances from the two populations being compared.\n","Under the null hypothesis (that the population variances are equal), this ratio should follow an F-distribution with degrees of freedom based on the sample sizes. Therefore, it is assumed that the populations being compared have equal variances under the null hypothesis (i.e., the test is comparing the hypothesis\n","ðœŽ\n","1\n","2\n","=\n","ðœŽ\n","2\n","2\n","Ïƒ\n","1\n","2\n","â€‹\n"," =Ïƒ\n","2\n","2\n","â€‹\n"," ).\n","5. Scale of Measurement\n","The data should be measured on a continuous scale, such as interval or ratio scales. The variance is a measure of the spread of continuous data, and the F-test is designed to compare variances of such data.\n","6. Sample Size and Variance Estimation\n","The sample sizes should ideally be large enough to provide reliable estimates of the population variances. For small sample sizes, the F-distribution may not approximate the true distribution well, making the test less reliable.\n","It is also important that the sample variances are good estimates of the population variances. This assumption is satisfied if the samples are random and drawn independently from the populations.\n","7. Homogeneity of Variances (Under the Null Hypothesis)\n","The null hypothesis of the F-test assumes that the variances of the two populations are equal (\n","ð»\n","0\n",":\n","ðœŽ\n","1\n","2\n","=\n","ðœŽ\n","2\n","2\n","H\n","0\n","â€‹\n"," :Ïƒ\n","1\n","2\n","â€‹\n"," =Ïƒ\n","2\n","2\n","â€‹\n"," ).\n","The alternative hypothesis typically assumes that the variances are not equal (\n","ð»\n","1\n",":\n","ðœŽ\n","1\n","2\n","â‰ \n","ðœŽ\n","2\n","2\n","H\n","1\n","â€‹\n"," :Ïƒ\n","1\n","2\n","â€‹\n","\n","î€ \n","=Ïƒ\n","2\n","2\n","â€‹\n"," ).\n","Violations of Assumptions and Their Impact:\n","Non-Normality: If the data are not normally distributed, the F-test may not be valid, especially for small sample sizes. In such cases, alternative tests (such as Levene's test or Bartlett's test) can sometimes be used, which are less sensitive to normality assumptions.\n","\n","Small Sample Sizes: The F-test can be quite sensitive to small sample sizes. If both sample sizes are small (e.g., less than 20), the normality assumption becomes more critical, and the test may yield unreliable results. In this case, bootstrapping methods or non-parametric alternatives may be better options.\n","\n","Unequal Sample Sizes: While the F-test can be used with unequal sample sizes, the sensitivity to violations of normality increases when sample sizes are not equal. Larger sample sizes are needed in such cases to achieve reliable results."],"metadata":{"id":"3_RAPyENZDF3"}},{"cell_type":"markdown","source":["Purpose of ANOVA (Analysis of Variance):\n","\n","The purpose of ANOVA (Analysis of Variance) is to compare the means of three or more groups to determine whether there is a statistically significant difference among them. ANOVA tests the null hypothesis that all group means are equal, against the alternative hypothesis that at least one group mean is different.\n","\n","In simpler terms, ANOVA helps answer questions like:\n","\n","\"Do different treatment groups lead to different outcomes?\"\n","\"Is there a significant difference in the average scores of multiple groups?\"\n","\"Is there variability in data that can be attributed to different group factors?\"\n","ANOVA works by analyzing the variance within and between groups:\n","\n","Between-group variance measures the variability in group means compared to the overall mean.\n","Within-group variance measures the variability within each group.\n","By comparing these two sources of variance, ANOVA helps determine whether the differences between group means are large enough to be considered statistically significant or if they are simply due to random variation.\n","\n","Key Components of ANOVA:\n","\n","Between-group variation: Variability in the means of different groups.\n","Within-group variation: Variability within each group (or how much the individual observations deviate from their group mean).\n","\n","F-statistic: The ratio of between-group variance to within-group variance. A large F-statistic suggests that the group means are significantly different, while a small F-statistic suggests no significant difference.\n","\n","Types of ANOVA:\n","\n","One-way ANOVA: Compares the means of three or more groups based on one independent variable (factor).\n","\n","Two-way ANOVA: Compares the means of groups based on two independent variables (factors) and can also test for interaction effects between them.\n","\n","Repeated measures ANOVA: Used when the same subjects are measured multiple times (e.g., before and after a treatment).\n","\n","Difference Between ANOVA and t-test:\n","\n","Both ANOVA and t-tests are used to compare means, but they differ in the number of groups they can handle and the way they approach hypothesis testing. Below are the key differences:\n","\n","1. Number of Groups Compared:\n","ANOVA is used when comparing three or more groups. It tests whether there is at least one significant difference among the means of multiple groups.\n","t-test is used to compare the means of two groups.\n","2. Test Hypothesis:\n","ANOVA tests the null hypothesis that all group means are equal. If the null hypothesis is rejected, it indicates that there is at least one group that is significantly different from the others, but ANOVA doesnâ€™t tell us which specific groups are different.\n","Null hypothesis for ANOVA:\n","ð»\n","0\n",":\n","ðœ‡\n","1\n","=\n","ðœ‡\n","2\n","=\n","â‹¯\n","=\n","ðœ‡\n","ð‘˜\n","H\n","0\n","â€‹\n"," :Î¼\n","1\n","â€‹\n"," =Î¼\n","2\n","â€‹\n"," =â‹¯=Î¼\n","k\n","â€‹\n","  (where\n","ð‘˜\n","k is the number of groups).\n","t-test tests the null hypothesis that two group means are equal.\n","Null hypothesis for t-test:\n","ð»\n","0\n",":\n","ðœ‡\n","1\n","=\n","ðœ‡\n","2\n","H\n","0\n","â€‹\n"," :Î¼\n","1\n","â€‹\n"," =Î¼\n","2\n","â€‹\n"," .\n","3. Multiple Comparisons:\n","ANOVA allows for testing multiple groups simultaneously, which helps to control the Type I error rate (the risk of incorrectly rejecting the null hypothesis). If you perform multiple t-tests, the probability of incorrectly finding a significant result (Type I error) increases, which is known as the problem of multiple comparisons.\n","t-test is limited to comparing only two groups at a time. If you have more than two groups, you would need to perform multiple pairwise t-tests, which increases the chance of making a Type I error.\n","4. Type of Data:\n","ANOVA is generally used when you have categorical independent variables (factors) and continuous dependent variables (e.g., comparing the means of different treatment groups).\n","t-test is used when you have two groups and you want to compare their means (for example, comparing the average weight of men vs. women).\n","5. Post-Hoc Testing:\n","ANOVA will tell you whether there is a significant difference among the groups as a whole, but it does not tell you which specific groups differ from each other. After conducting ANOVA and finding a significant result, you often need to perform post-hoc tests (e.g., Tukeyâ€™s HSD) to determine which groups are different.\n","t-test gives a direct answer about the difference between two groups, but when comparing more than two groups, multiple t-tests are needed, which can be problematic (as mentioned above).\n","6. Calculation:\n","ANOVA uses the F-statistic, which is the ratio of between-group variance to within-group variance.\n","t-test uses the t-statistic, which is a measure of the difference between two group means relative to the variability within the groups.\n","When to Use Each:\n","Use ANOVA when:\n","\n","We have three or more groups to compare.\n","We want to test the overall significance of group differences without committing to multiple pairwise comparisons.\n","We are concerned with controlling Type I error across multiple comparisons.\n","Use a t-test when:\n","\n","We are comparing the means of exactly two groups.\n","We need a more straightforward test for comparing two groups' means.\n","Example:\n","ANOVA Example: We want to test if there is a difference in the average exam scores between three teaching methods (Method A, Method B, and Method C). Since you are comparing more than two groups, ANOVA is appropriate.\n","\n","t-test Example: We want to test if the average exam score between male and female students is different. Since you are comparing only two groups (males vs. females), a t-test is appropriate.\n","\n"],"metadata":{"id":"H44Dc7xzZ2-Y"}},{"cell_type":"markdown","source":["When comparing the means of more than two groups, it is generally more appropriate to use one-way ANOVA rather than conducting multiple t-tests. Hereâ€™s an explanation of why and when you would choose a one-way ANOVA over multiple t-tests:\n","\n","Why Use One-Way ANOVA Instead of Multiple t-tests?\n","Control of Type I Error (Inflated Error Rate):\n","\n","The most important reason to use ANOVA instead of multiple t-tests is to control the Type I error rate (the probability of incorrectly rejecting the null hypothesis when it is true).\n","Each time you conduct a t-test, you are setting a significance level (e.g.,\n","ð›¼\n","=\n","0.05\n","Î±=0.05), which means there is a 5% chance of rejecting the null hypothesis if it is true (a Type I error).\n","When you perform multiple t-tests, the overall probability of making at least one Type I error increases. This is known as the problem of multiple comparisons or inflated Type I error.\n","For example, if you perform three t-tests (comparing three groups), the probability of committing at least one Type I error is greater than 0.05 (it could be around 0.14 with 3 tests, assuming each t-test has a 5% significance level).\n","ANOVA, on the other hand, allows you to test for differences across multiple groups with a single test, thus controlling the overall Type I error rate at the desired significance level (e.g., 5%).\n","Efficiency:\n","\n","One-way ANOVA is much more efficient than running multiple t-tests because it compares all groups simultaneously, reducing the need for multiple hypothesis tests and computations.\n","With multiple t-tests, you would have to adjust for the increased error rate (using techniques like the Bonferroni correction or Holm's correction), which makes the process more complex and reduces the power of the tests (i.e., the ability to detect a true effect).\n","ANOVA computes an F-statistic, which evaluates whether there is a significant difference among all group means in a single step, without needing to perform pairwise comparisons unless the overall test is significant.\n","Holistic Comparison Across Groups:\n","\n","ANOVA tests the null hypothesis that all group means are equal (i.e., there is no significant difference among any of the groups), which provides a global comparison.\n","In contrast, a t-test only compares two groups at a time. When you have more than two groups, this method doesnâ€™t give you a clear picture of the overall variation across all groups.\n","If the ANOVA result is significant (i.e., the null hypothesis is rejected), you can then conduct post-hoc pairwise comparisons (e.g., Tukeyâ€™s HSD or Bonferroni) to identify which specific group differences are significant. This avoids the problem of making multiple independent decisions about group differences.\n","Handling of Complex Data:\n","\n","ANOVA works directly with the variances of the groups, not just the means. By comparing the between-group variance to the within-group variance, ANOVA can provide a more robust test for group differences, particularly when there is heterogeneity among groups.\n","In contrast, t-tests only focus on differences in means, and comparing more than two groups with multiple t-tests could lead to an inaccurate or misleading understanding of the data.\n","When to Use One-Way ANOVA:\n","You should use one-way ANOVA when:\n","\n","You are comparing the means of three or more groups on a single factor (independent variable). For example, if you want to compare the average test scores of students from three different teaching methods (Method A, Method B, and Method C), one-way ANOVA is appropriate.\n","You are interested in testing a single factor (independent variable) and its effect on a continuous dependent variable. For example, testing if different diets (3 types: Diet 1, Diet 2, Diet 3) have an impact on weight loss.\n","You want to make a single, overall decision about whether there is any significant difference in the group means, rather than making multiple pairwise comparisons.\n","Example Scenario:\n","Imagine you want to test if three different diets (Diet A, Diet B, and Diet C) lead to different average weight loss after 6 months. Using multiple t-tests would involve testing:\n","\n","ð‘¡\n","t-test comparing Diet A vs. Diet B,\n","ð‘¡\n","t-test comparing Diet A vs. Diet C,\n","ð‘¡\n","t-test comparing Diet B vs. Diet C.\n","If you conduct these tests, the probability of making a Type I error increases with each test. In contrast, one-way ANOVA will give you a single test statistic (the F-statistic) that evaluates whether there is any significant difference in the means of the three diets simultaneously. If the result is significant, you can proceed to post-hoc tests to find out which specific diet(s) differ from others.\n","\n","When Not to Use One-Way ANOVA:\n","There are some cases where one-way ANOVA might not be the best choice:\n","\n","If you only have two groups to compare, then a t-test is more appropriate. For example, if you were comparing the effectiveness of two teaching methods, using a t-test would be the right choice.\n","If the data violates assumptions of ANOVA, such as:\n","Non-normality in the data (ANOVA assumes that the data in each group are normally distributed).\n"],"metadata":{"id":"y0RFRyvRa7LE"}},{"cell_type":"markdown","source":["In Analysis of Variance (ANOVA), variance is partitioned into two components: between-group variance and within-group variance. This partitioning is key to understanding how ANOVA tests for differences between group means. The F-statistic in ANOVA is derived from the ratio of these two sources of variance, providing a measure of whether the variation between the groups is large enough relative to the variation within the groups to suggest a statistically significant difference.\n","\n","1. Total Variance:\n","The total variance in the dataset represents the overall spread or variability of all the observations around the grand mean (the mean of all the data, regardless of group membership).\n","This total variance is calculated using the sum of squared deviations from the grand mean:\n","TotalÂ SumÂ ofÂ SquaresÂ (SST)\n","=\n","âˆ‘\n","ð‘–\n","=\n","1\n","ð‘›\n","(\n","ð‘Œ\n","ð‘–\n","âˆ’\n","ð‘Œ\n","Ë‰\n","grand\n",")\n","2\n","TotalÂ SumÂ ofÂ SquaresÂ (SST)=\n","i=1\n","âˆ‘\n","n\n","â€‹\n"," (Y\n","i\n","â€‹\n"," âˆ’\n","Y\n","Ë‰\n","  \n","grand\n","â€‹\n"," )\n","2\n","\n","where\n","ð‘Œ\n","ð‘–\n","Y\n","i\n","â€‹\n","  is an individual observation, and\n","ð‘Œ\n","Ë‰\n","grand\n","Y\n","Ë‰\n","  \n","grand\n","â€‹\n","  is the overall mean (grand mean).\n","2. Partitioning the Total Variance:\n","In ANOVA, the total variance is partitioned into two components: between-group variance and within-group variance. This partitioning allows us to evaluate whether the differences between group means are more significant than the variability within the groups.\n","\n","a. Between-Group Variance (Explained Variance):\n","The between-group variance reflects how much the group means deviate from the overall mean (grand mean). It measures the variation due to the differences between the groups.\n","If the group means are different, the between-group variance will be large, indicating that the group means are spread out from the grand mean.\n","The between-group sum of squares (SSB) is calculated by:\n","SumÂ ofÂ SquaresÂ BetweenÂ (SSB)\n","=\n","âˆ‘\n","ð‘—\n","=\n","1\n","ð‘˜\n","ð‘›\n","ð‘—\n","(\n","ð‘Œ\n","Ë‰\n","ð‘—\n","âˆ’\n","ð‘Œ\n","Ë‰\n","grand\n",")\n","2\n","SumÂ ofÂ SquaresÂ BetweenÂ (SSB)=\n","j=1\n","âˆ‘\n","k\n","â€‹\n"," n\n","j\n","â€‹\n"," (\n","Y\n","Ë‰\n","  \n","j\n","â€‹\n"," âˆ’\n","Y\n","Ë‰\n","  \n","grand\n","â€‹\n"," )\n","2\n","\n","where:\n","ð‘˜\n","k is the number of groups,\n","ð‘›\n","ð‘—\n","n\n","j\n","â€‹\n","  is the number of observations in the\n","ð‘—\n","j-th group,\n","ð‘Œ\n","Ë‰\n","ð‘—\n","Y\n","Ë‰\n","  \n","j\n","â€‹\n","  is the mean of the\n","ð‘—\n","j-th group,\n","ð‘Œ\n","Ë‰\n","grand\n","Y\n","Ë‰\n","  \n","grand\n","â€‹\n","  is the overall mean of all observations.\n","The mean square between (MSB) is obtained by dividing the sum of squares between (SSB) by its degrees of freedom (df):\n","\n","MSB\n","=\n","SSB\n","ð‘˜\n","âˆ’\n","1\n","MSB=\n","kâˆ’1\n","SSB\n","â€‹\n","\n","where\n","ð‘˜\n","âˆ’\n","1\n","kâˆ’1 is the degrees of freedom associated with between-group variance.\n","\n","b. Within-Group Variance (Unexplained Variance):\n","The within-group variance represents the variation within each group. It captures the random error or unexplained variation that cannot be attributed to the differences between the groups.\n","Each group has its own variation around its mean, and this within-group variation is averaged across all groups to get the mean square within (MSW).\n","The within-group sum of squares (SSW) is calculated by:\n","SumÂ ofÂ SquaresÂ WithinÂ (SSW)\n","=\n","âˆ‘\n","ð‘—\n","=\n","1\n","ð‘˜\n","âˆ‘\n","ð‘–\n","=\n","1\n","ð‘›\n","ð‘—\n","(\n","ð‘Œ\n","ð‘–\n","ð‘—\n","âˆ’\n","ð‘Œ\n","Ë‰\n","ð‘—\n",")\n","2\n","SumÂ ofÂ SquaresÂ WithinÂ (SSW)=\n","j=1\n","âˆ‘\n","k\n","â€‹\n","  \n","i=1\n","âˆ‘\n","n\n","j\n","â€‹\n","\n","â€‹\n"," (Y\n","ij\n","â€‹\n"," âˆ’\n","Y\n","Ë‰\n","  \n","j\n","â€‹\n"," )\n","2\n","\n","where:\n","ð‘Œ\n","ð‘–\n","ð‘—\n","Y\n","ij\n","â€‹\n","  is the\n","ð‘–\n","i-th observation in group\n","ð‘—\n","j,\n","ð‘Œ\n","Ë‰\n","ð‘—\n","Y\n","Ë‰\n","  \n","j\n","â€‹\n","  is the mean of group\n","ð‘—\n","j.\n","The mean square within (MSW) is obtained by dividing the sum of squares within (SSW) by its degrees of freedom (df):\n","\n","MSW\n","=\n","SSW\n","ð‘›\n","âˆ’\n","ð‘˜\n","MSW=\n","nâˆ’k\n","SSW\n","â€‹\n","\n","where\n","ð‘›\n","âˆ’\n","ð‘˜\n","nâˆ’k is the degrees of freedom associated with within-group variance, and\n","ð‘›\n","n is the total number of observations across all groups.\n","\n","3. The F-statistic:\n","The F-statistic in ANOVA is a ratio of the between-group variance to the within-group variance:\n","\n","ð¹\n","=\n","MSB\n","MSW\n","=\n","SSB\n","ð‘˜\n","âˆ’\n","1\n","SSW\n","ð‘›\n","âˆ’\n","ð‘˜\n","F=\n","MSW\n","MSB\n","â€‹\n"," =\n","nâˆ’k\n","SSW\n","â€‹\n","\n","kâˆ’1\n","SSB\n","â€‹\n","\n","â€‹\n","\n","where:\n","\n","MSB (Mean Square Between) represents the variation explained by the differences between the group means (systematic variation),\n","MSW (Mean Square Within) represents the variation within the groups (random variation or error).\n","4. Interpretation of the F-statistic:\n","If the group means are similar and the within-group variance is large (i.e., there's a lot of variation within each group), the F-statistic will be small (close to 1).\n","If the group means are very different from each other, with little variation within each group, the F-statistic will be large.\n","A large F-statistic indicates that the variability between the groups is much greater than the variability within the groups, suggesting that there is a significant difference between the group means. Conversely, a small F-statistic suggests that the group means are similar, and any differences observed are likely due to random error rather than systematic effects.\n","5. Degrees of Freedom:\n","Degrees of freedom for between-group variance:\n","ð‘‘\n","ð‘“\n","between\n","=\n","ð‘˜\n","âˆ’\n","1\n","df\n","between\n","â€‹\n"," =kâˆ’1, where\n","ð‘˜\n","k is the number of groups.\n","Degrees of freedom for within-group variance:\n","ð‘‘\n","ð‘“\n","within\n","=\n","ð‘›\n","âˆ’\n","ð‘˜\n","df\n","within\n","â€‹\n"," =nâˆ’k, where\n","ð‘›\n","n is the total number of observations across all groups.\n","6. F-distribution:\n","The F-statistic follows an F-distribution with degrees of freedom\n","ð‘‘\n","ð‘“\n","1\n","=\n","ð‘˜\n","âˆ’\n","1\n","df\n","1\n","â€‹\n"," =kâˆ’1 (for between-group variance) and\n","ð‘‘\n","ð‘“\n","2\n","=\n","ð‘›\n","âˆ’\n","ð‘˜\n","df\n","2\n","â€‹\n"," =nâˆ’k (for within-group variance). The F-distribution is used to determine the critical value of F at a given significance level (e.g.,\n","ð›¼\n","=\n","0.05\n","Î±=0.05).\n","If the calculated F-statistic is greater than the critical F-value from the F-distribution, we reject the null hypothesis and conclude that there are significant differences between the group means.\n"],"metadata":{"id":"BkOCs0WVbdOX"}},{"cell_type":"markdown","source":["The classical (frequentist) approach and the Bayesian approach to ANOVA are two distinct methodologies for analyzing data and interpreting uncertainty. While both methods aim to test hypotheses and estimate parameters, they differ fundamentally in how they handle uncertainty, parameter estimation, and hypothesis testing. Below is a comparison between the two approaches in terms of these aspects.\n","\n","1. Handling of Uncertainty:\n","Frequentist Approach:\n","In the classical (frequentist) approach, uncertainty is viewed in terms of sampling variability. The parameters (e.g., group means, variances) are considered fixed but unknown quantities.\n","Uncertainty is quantified using confidence intervals and p-values, which provide estimates of how likely an observed result is given a specific set of assumptions (e.g., normality, independence).\n","Frequentist methods assume that if the experiment were repeated an infinite number of times, the parameter estimates would converge to the true population parameters. Thus, uncertainty is based on repeated sampling.\n","Bayesian Approach:\n","In the Bayesian approach, uncertainty is represented by probability distributions. Instead of assuming that parameters are fixed but unknown, Bayesian methods treat parameters as random variables that can be described by probability distributions.\n","Uncertainty is modeled using prior distributions (which represent initial beliefs about the parameters before seeing the data) and posterior distributions (which represent updated beliefs about the parameters after observing the data).\n","The posterior distribution provides a direct way to quantify the uncertainty around parameter estimates, allowing for richer insights into parameter uncertainty (e.g., posterior means, credible intervals).\n","2. Parameter Estimation:\n","Frequentist Approach:\n","In frequentist ANOVA, parameters (e.g., group means) are estimated using point estimates (e.g., sample means) and the uncertainty around these estimates is quantified using standard errors or confidence intervals.\n","The key parameter in ANOVA is the F-statistic, which is used to test the null hypothesis (that all group means are equal). The least squares method is typically used to estimate the group means and variances.\n","Frequentist parameter estimates do not incorporate prior information; they are based solely on the data at hand and are considered the best estimate given the data.\n","Bayesian Approach:\n","In Bayesian ANOVA, parameters (such as group means, variances, or differences between group means) are estimated using posterior distributions. These distributions are derived by combining prior beliefs (prior distributions) with the likelihood of the data under a particular model.\n","Instead of a single point estimate, the Bayesian approach provides a distribution of possible values for each parameter, which offers a more comprehensive view of uncertainty.\n","Parameters in a Bayesian context are treated as random variables, and the goal is to calculate the posterior distribution using Bayes' theorem:\n","ð‘ƒ\n","(\n","ðœƒ\n","âˆ£\n","data\n",")\n","âˆ\n","ð‘ƒ\n","(\n","data\n","âˆ£\n","ðœƒ\n",")\n","ð‘ƒ\n","(\n","ðœƒ\n",")\n","P(Î¸âˆ£data)âˆP(dataâˆ£Î¸)P(Î¸)\n","where\n","ð‘ƒ\n","(\n","ðœƒ\n","âˆ£\n","data\n",")\n","P(Î¸âˆ£data) is the posterior distribution,\n","ð‘ƒ\n","(\n","data\n","âˆ£\n","ðœƒ\n",")\n","P(dataâˆ£Î¸) is the likelihood of the data, and\n","ð‘ƒ\n","(\n","ðœƒ\n",")\n","P(Î¸) is the prior distribution for the parameters.\n","3. Hypothesis Testing:\n","Frequentist Approach:\n","In frequentist ANOVA, hypothesis testing is conducted using the null hypothesis significance testing (NHST) framework.\n","The null hypothesis in ANOVA typically states that all group means are equal (\n","ð»\n","0\n",":\n","ðœ‡\n","1\n","=\n","ðœ‡\n","2\n","=\n","â‹¯\n","=\n","ðœ‡\n","ð‘˜\n","H\n","0\n","â€‹\n"," :Î¼\n","1\n","â€‹\n"," =Î¼\n","2\n","â€‹\n"," =â‹¯=Î¼\n","k\n","â€‹\n"," ), and the alternative hypothesis states that at least one group mean is different.\n","The test statistic (the F-statistic) is calculated, and a p-value is computed to determine the strength of evidence against the null hypothesis. If the p-value is less than a pre-specified significance level (e.g., 0.05), the null hypothesis is rejected, indicating a statistically significant difference between group means.\n","The frequentist approach does not provide direct probability statements about the parameters themselves; it only tells you the probability of observing the data given the null hypothesis.\n","Bayesian Approach:\n","In the Bayesian framework, hypothesis testing is conducted by comparing posterior probabilities for different hypotheses. Rather than using a p-value, Bayesian hypothesis testing typically involves calculating the posterior probability of each hypothesis or model and comparing them.\n","One common approach is to compute the Bayes factor, which is the ratio of the likelihood of the data under two competing hypotheses (e.g., the null hypothesis vs. the alternative hypothesis):\n","BayesÂ Factor\n","=\n","ð‘ƒ\n","(\n","data\n","âˆ£\n","ð»\n","1\n",")\n","ð‘ƒ\n","(\n","data\n","âˆ£\n","ð»\n","0\n",")\n","BayesÂ Factor=\n","P(dataâˆ£H\n","0\n","â€‹\n"," )\n","P(dataâˆ£H\n","1\n","â€‹\n"," )\n","â€‹\n","\n","where\n","ð»\n","0\n","H\n","0\n","â€‹\n","  is the null hypothesis and\n","ð»\n","1\n","H\n","1\n","â€‹\n","  is the alternative hypothesis. A Bayes factor greater than 1 suggests evidence in favor of\n","ð»\n","1\n","H\n","1\n","â€‹\n"," , while a value less than 1 suggests evidence in favor of\n","ð»\n","0\n","H\n","0\n","â€‹\n"," .\n","Bayesian hypothesis testing does not rely on rejecting or failing to reject the null hypothesis but instead provides a probabilistic interpretation of the evidence in favor of or against a hypothesis.\n","4. Interpretation of Results:\n","Frequentist Approach:\n","The frequentist interpretation of the results focuses on rejecting or failing to reject the null hypothesis. A p-value tells you whether there is enough evidence to support rejecting the null hypothesis.\n","The confidence intervals give you a range of values within which the true population parameters are likely to lie, based on the data.\n","Interpretation of p-values: A p-value of 0.05 indicates that if the null hypothesis were true, there is a 5% chance of observing the data or something more extreme. However, a p-value does not directly give the probability that the null hypothesis is true.\n","Bayesian Approach:\n","The Bayesian interpretation provides a probabilistic statement about the parameters themselves. The posterior distribution reflects our updated beliefs about the parameters after observing the data, incorporating prior knowledge and the observed data.\n","For example, you can directly calculate the probability that a group mean is greater than a certain value, or the probability that a difference between two group means is statistically significant.\n","The credible interval in Bayesian analysis is analogous to the confidence interval in frequentist analysis, but it provides a probabilistic statement. For example, a 95% credible interval for a group mean means that there is a 95% probability that the true group mean lies within that interval, given the data and the prior information.\n"],"metadata":{"id":"TkY1Js7cb3Ye"}},{"cell_type":"code","source":["import numpy as np\n","from scipy import stats\n","\n","# Given data\n","profession_a = np.array([48, 52, 55, 60, 62])\n","profession_b = np.array([45, 50, 55, 52, 47])\n","\n","# Calculate variances for both groups\n","var_a = np.var(profession_a, ddof=1)  # Sample variance for Profession A\n","var_b = np.var(profession_b, ddof=1)  # Sample variance for Profession B\n","\n","# Calculate the F-statistic\n","f_statistic = var_a / var_b if var_a > var_b else var_b / var_a\n","\n","# Degrees of freedom\n","df1 = len(profession_a) - 1  # Degrees of freedom for Profession A\n","df2 = len(profession_b) - 1  # Degrees of freedom for Profession B\n","\n","# Calculate the p-value using the F-distribution\n","p_value = 2 * min(stats.f.cdf(f_statistic, df1, df2), 1 - stats.f.cdf(f_statistic, df1, df2))\n","\n","# Output the results\n","print(f\"Variance of Profession A: {var_a}\")\n","print(f\"Variance of Profession B: {var_b}\")\n","print(f\"F-statistic: {f_statistic}\")\n","print(f\"P-value: {p_value}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q3P4BsTKcxNo","executionInfo":{"status":"ok","timestamp":1731600832930,"user_tz":-330,"elapsed":4091,"user":{"displayName":"Varun Garg","userId":"11663094782443047067"}},"outputId":"d735a7c9-747e-4179-ae04-b654da042af0"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Variance of Profession A: 32.8\n","Variance of Profession B: 15.7\n","F-statistic: 2.089171974522293\n","P-value: 0.49304859900533904\n"]}]},{"cell_type":"code","source":["import numpy as np\n","from scipy import stats\n","\n","# Given data\n","region_a = np.array([160, 162, 165, 158, 164])\n","region_b = np.array([172, 175, 170, 168, 174])\n","region_c = np.array([180, 182, 179, 185, 183])\n","\n","# Perform one-way ANOVA\n","f_statistic, p_value = stats.f_oneway(region_a, region_b, region_c)\n","\n","# Output the results\n","print(f\"F-statistic: {f_statistic}\")\n","print(f\"P-value: {p_value}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rNddbl62c1fL","executionInfo":{"status":"ok","timestamp":1731600897651,"user_tz":-330,"elapsed":625,"user":{"displayName":"Varun Garg","userId":"11663094782443047067"}},"outputId":"83fcb09e-46b4-4359-e45e-1bfaade9a469"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["F-statistic: 67.87330316742101\n","P-value: 2.870664187937026e-07\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"0OmsvSckdMui"},"execution_count":null,"outputs":[]}]}